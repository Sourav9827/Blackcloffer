{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b7e46f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, string\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0d30bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '../StopWords'\n",
    "stopwords_articles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d585bf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(os.path.join(folder_path, filename), 'r') as file:\n",
    "            stopwords_articles.extend(file.read().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "730ce0b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ERNST',\n",
       " 'YOUNG',\n",
       " 'DELOITTE',\n",
       " 'TOUCHE',\n",
       " 'KPMG',\n",
       " 'PRICEWATERHOUSECOOPERS',\n",
       " 'PRICEWATERHOUSE',\n",
       " 'COOPERS',\n",
       " 'AFGHANI',\n",
       " '|',\n",
       " 'Afghanistan',\n",
       " 'ARIARY',\n",
       " '|',\n",
       " 'Madagascar',\n",
       " 'BAHT',\n",
       " '|',\n",
       " 'Thailand',\n",
       " 'BALBOA',\n",
       " '|',\n",
       " 'Panama',\n",
       " 'BIRR',\n",
       " '|',\n",
       " 'Ethiopia',\n",
       " 'BOLIVAR',\n",
       " '|',\n",
       " 'Venezuela',\n",
       " 'BOLIVIANO',\n",
       " '|',\n",
       " 'Bolivia',\n",
       " 'CEDI',\n",
       " '|',\n",
       " 'Ghana',\n",
       " 'COLON',\n",
       " '|',\n",
       " 'Costa',\n",
       " 'Rica',\n",
       " 'CÓRDOBA',\n",
       " '|',\n",
       " 'Nicaragua',\n",
       " 'DALASI',\n",
       " '|',\n",
       " 'Gambia',\n",
       " 'DENAR',\n",
       " '|',\n",
       " 'Macedonia',\n",
       " '(Former',\n",
       " 'Yug.',\n",
       " 'Rep.)',\n",
       " 'DINAR',\n",
       " '|',\n",
       " 'Algeria',\n",
       " 'DIRHAM',\n",
       " '|',\n",
       " 'Morocco',\n",
       " 'DOBRA',\n",
       " '|',\n",
       " 'São',\n",
       " 'Tom',\n",
       " 'and',\n",
       " 'Príncipe',\n",
       " 'DONG',\n",
       " '|',\n",
       " 'Vietnam',\n",
       " 'DRAM',\n",
       " '|',\n",
       " 'Armenia',\n",
       " 'ESCUDO',\n",
       " '|',\n",
       " 'Cape',\n",
       " 'Verde',\n",
       " 'EURO',\n",
       " '|',\n",
       " 'Belgium',\n",
       " 'FLORIN',\n",
       " '|',\n",
       " 'Aruba',\n",
       " 'FORINT',\n",
       " '|',\n",
       " 'Hungary',\n",
       " 'GOURDE',\n",
       " '|',\n",
       " 'Haiti',\n",
       " 'GUARANI',\n",
       " '|',\n",
       " 'Paraguay',\n",
       " 'GULDEN',\n",
       " '|',\n",
       " 'Netherlands',\n",
       " 'Antilles',\n",
       " 'HRYVNIA',\n",
       " '|',\n",
       " 'Ukraine',\n",
       " 'KINA',\n",
       " '|',\n",
       " 'Papua',\n",
       " 'New',\n",
       " 'Guinea',\n",
       " 'KIP',\n",
       " '|',\n",
       " 'Laos',\n",
       " 'KONVERTIBILNA',\n",
       " 'MARKA',\n",
       " '|',\n",
       " 'Bosnia-Herzegovina',\n",
       " 'KORUNA',\n",
       " '|',\n",
       " 'Czech',\n",
       " 'Republic',\n",
       " 'KRONA',\n",
       " '|',\n",
       " 'Sweden',\n",
       " 'KRONE',\n",
       " '|',\n",
       " 'Denmark',\n",
       " 'KROON',\n",
       " '|',\n",
       " 'Estonia',\n",
       " 'KUNA',\n",
       " '|',\n",
       " 'Croatia',\n",
       " 'KWACHA',\n",
       " '|',\n",
       " 'Zambia',\n",
       " 'KWANZA',\n",
       " '|',\n",
       " 'Angola',\n",
       " 'KYAT',\n",
       " '|',\n",
       " 'Myanmar',\n",
       " 'LARI',\n",
       " '|',\n",
       " 'Georgia',\n",
       " 'LATS',\n",
       " '|',\n",
       " 'Latvia',\n",
       " 'LEK',\n",
       " '|',\n",
       " 'Albania',\n",
       " 'LEMPIRA',\n",
       " '|',\n",
       " 'Honduras',\n",
       " 'LEONE',\n",
       " '|',\n",
       " 'Sierra',\n",
       " 'Leone',\n",
       " 'LEU',\n",
       " '|',\n",
       " 'Romania',\n",
       " 'LEV',\n",
       " '|',\n",
       " 'Bulgaria',\n",
       " 'LILANGENI',\n",
       " '|',\n",
       " 'Swaziland',\n",
       " 'LIRA',\n",
       " '|',\n",
       " 'Lebanon',\n",
       " 'LITAS',\n",
       " '|',\n",
       " 'Lithuania',\n",
       " 'LOTI',\n",
       " '|',\n",
       " 'Lesotho',\n",
       " 'MANAT',\n",
       " '|',\n",
       " 'Azerbaijan',\n",
       " 'METICAL',\n",
       " '|',\n",
       " 'Mozambique',\n",
       " 'NAIRA',\n",
       " '|',\n",
       " 'Nigeria',\n",
       " 'NAKFA',\n",
       " '|',\n",
       " 'Eritrea',\n",
       " 'NEW',\n",
       " 'LIRA',\n",
       " '|',\n",
       " 'Turkey',\n",
       " 'NEW',\n",
       " 'SHEQEL',\n",
       " '|',\n",
       " 'Israel',\n",
       " 'NGULTRUM',\n",
       " '|',\n",
       " 'Bhutan',\n",
       " 'NUEVO',\n",
       " 'SOL',\n",
       " '|',\n",
       " 'Peru',\n",
       " 'OUGUIYA',\n",
       " '|',\n",
       " 'Mauritania',\n",
       " 'PATACA',\n",
       " '|',\n",
       " 'Macau',\n",
       " 'PESO',\n",
       " '|',\n",
       " 'Mexico',\n",
       " 'POUND',\n",
       " '|',\n",
       " 'Egypt',\n",
       " 'PULA',\n",
       " '|',\n",
       " 'Botswana',\n",
       " 'QUETZAL',\n",
       " '|',\n",
       " 'Guatemala',\n",
       " 'RAND',\n",
       " '|',\n",
       " 'South',\n",
       " 'Africa',\n",
       " 'REAL',\n",
       " '|',\n",
       " 'Brazil',\n",
       " 'RENMINBI',\n",
       " '|',\n",
       " 'China',\n",
       " 'RIAL',\n",
       " '|',\n",
       " 'Iran',\n",
       " 'RIEL',\n",
       " '|',\n",
       " 'Cambodia',\n",
       " 'RINGGIT',\n",
       " '|',\n",
       " 'Malaysia',\n",
       " 'RIYAL',\n",
       " '|',\n",
       " 'Saudi',\n",
       " 'Arabia',\n",
       " 'RUBLE',\n",
       " '|',\n",
       " 'Russia',\n",
       " 'RUFIYAA',\n",
       " '|',\n",
       " 'Maldives',\n",
       " 'RUPEE',\n",
       " '|',\n",
       " 'India',\n",
       " 'RUPEE',\n",
       " '|',\n",
       " 'Pakistan',\n",
       " 'RUPIAH',\n",
       " '|',\n",
       " 'Indonesia',\n",
       " 'SHILLING',\n",
       " '|',\n",
       " 'Uganda',\n",
       " 'SOM',\n",
       " '|',\n",
       " 'Uzbekistan',\n",
       " 'SOMONI',\n",
       " '|',\n",
       " 'Tajikistan',\n",
       " 'SPECIAL',\n",
       " 'DRAWING',\n",
       " 'RIGHTS',\n",
       " '|',\n",
       " 'International',\n",
       " 'Monetary',\n",
       " 'Fund',\n",
       " 'TAKA',\n",
       " '|',\n",
       " 'Bangladesh',\n",
       " 'TALA',\n",
       " '|',\n",
       " 'Western',\n",
       " 'Samoa',\n",
       " 'TENGE',\n",
       " '|',\n",
       " 'Kazakhstan',\n",
       " 'TUGRIK',\n",
       " '|',\n",
       " 'Mongolia',\n",
       " 'VATU',\n",
       " '|',\n",
       " 'Vanuatu',\n",
       " 'WON',\n",
       " '|',\n",
       " 'Korea,',\n",
       " 'South',\n",
       " 'YEN',\n",
       " '|',\n",
       " 'Japan',\n",
       " 'ZLOTY',\n",
       " '|',\n",
       " 'Poland',\n",
       " 'HUNDRED',\n",
       " '|',\n",
       " 'Denominations',\n",
       " 'THOUSAND',\n",
       " 'MILLION',\n",
       " 'BILLION',\n",
       " 'TRILLION',\n",
       " 'DATE',\n",
       " '|',\n",
       " 'Time',\n",
       " 'related',\n",
       " 'ANNUAL',\n",
       " 'ANNUALLY',\n",
       " 'ANNUM',\n",
       " 'YEAR',\n",
       " 'YEARLY',\n",
       " 'QUARTER',\n",
       " 'QUARTERLY',\n",
       " 'QTR',\n",
       " 'MONTH',\n",
       " 'MONTHLY',\n",
       " 'WEEK',\n",
       " 'WEEKLY',\n",
       " 'DAY',\n",
       " 'DAILY',\n",
       " 'JANUARY',\n",
       " '|',\n",
       " 'Calendar',\n",
       " 'FEBRUARY',\n",
       " 'MARCH',\n",
       " 'APRIL',\n",
       " 'MAY',\n",
       " 'JUNE',\n",
       " 'JULY',\n",
       " 'AUGUST',\n",
       " 'SEPTEMBER',\n",
       " 'OCTOBER',\n",
       " 'NOVEMBER',\n",
       " 'DECEMBER',\n",
       " 'JAN',\n",
       " 'FEB',\n",
       " 'MAR',\n",
       " 'APR',\n",
       " 'MAY',\n",
       " 'JUN',\n",
       " 'JUL',\n",
       " 'AUG',\n",
       " 'SEP',\n",
       " 'SEPT',\n",
       " 'OCT',\n",
       " 'NOV',\n",
       " 'DEC',\n",
       " 'MONDAY',\n",
       " 'TUESDAY',\n",
       " 'WEDNESDAY',\n",
       " 'THURSDAY',\n",
       " 'FRIDAY',\n",
       " 'SATURDAY',\n",
       " 'SUNDAY',\n",
       " 'ONE',\n",
       " '|',\n",
       " 'Numbers',\n",
       " 'TWO',\n",
       " 'THREE',\n",
       " 'FOUR',\n",
       " 'FIVE',\n",
       " 'SIX',\n",
       " 'SEVEN',\n",
       " 'EIGHT',\n",
       " 'NINE',\n",
       " 'TEN',\n",
       " 'ELEVEN',\n",
       " 'TWELVE',\n",
       " 'THIRTEEN',\n",
       " 'FOURTEEN',\n",
       " 'FIFTEEN',\n",
       " 'SIXTEEN',\n",
       " 'SEVENTEEN',\n",
       " 'EIGHTEEN',\n",
       " 'NINETEEN',\n",
       " 'TWENTY',\n",
       " 'THIRTY',\n",
       " 'FORTY',\n",
       " 'FIFTY',\n",
       " 'SIXTY',\n",
       " 'SEVENTY',\n",
       " 'EIGHTY',\n",
       " 'NINETY',\n",
       " 'FIRST',\n",
       " 'SECOND',\n",
       " 'THIRD',\n",
       " 'FOURTH',\n",
       " 'FIFTH',\n",
       " 'SIXTH',\n",
       " 'SEVENTH',\n",
       " 'EIGHTH',\n",
       " 'NINTH',\n",
       " 'TENTH',\n",
       " 'I',\n",
       " '|',\n",
       " 'Roman',\n",
       " 'numerals',\n",
       " 'II',\n",
       " 'III',\n",
       " 'IV',\n",
       " 'V',\n",
       " 'VI',\n",
       " 'VII',\n",
       " 'VIII',\n",
       " 'IX',\n",
       " 'X',\n",
       " 'XI',\n",
       " 'XII',\n",
       " 'XIII',\n",
       " 'XIV',\n",
       " 'XV',\n",
       " 'XVI',\n",
       " 'XVII',\n",
       " 'XVIII',\n",
       " 'XIX',\n",
       " 'XX',\n",
       " 'ABOUT',\n",
       " 'ABOVE',\n",
       " 'AFTER',\n",
       " 'AGAIN',\n",
       " 'ALL',\n",
       " 'AM',\n",
       " 'AMONG',\n",
       " 'AN',\n",
       " 'AND',\n",
       " 'ANY',\n",
       " 'ARE',\n",
       " 'AS',\n",
       " 'AT',\n",
       " 'BE',\n",
       " 'BECAUSE',\n",
       " 'BEEN',\n",
       " 'BEFORE',\n",
       " 'BEING',\n",
       " 'BELOW',\n",
       " 'BETWEEN',\n",
       " 'BOTH',\n",
       " 'BUT',\n",
       " 'BY',\n",
       " 'CAN',\n",
       " 'DID',\n",
       " 'DO',\n",
       " 'DOES',\n",
       " 'DOING',\n",
       " 'DOWN',\n",
       " 'DURING',\n",
       " 'EACH',\n",
       " 'FEW',\n",
       " 'FOR',\n",
       " 'FROM',\n",
       " 'FURTHER',\n",
       " 'HAD',\n",
       " 'HAS',\n",
       " 'HAVE',\n",
       " 'HAVING',\n",
       " 'HE',\n",
       " 'HER',\n",
       " 'HERE',\n",
       " 'HERS',\n",
       " 'HERSELF',\n",
       " 'HIM',\n",
       " 'HIMSELF',\n",
       " 'HIS',\n",
       " 'HOW',\n",
       " 'IF',\n",
       " 'IN',\n",
       " 'INTO',\n",
       " 'IS',\n",
       " 'IT',\n",
       " 'ITS',\n",
       " 'ITSELF',\n",
       " 'JUST',\n",
       " 'ME',\n",
       " 'MORE',\n",
       " 'MOST',\n",
       " 'MY',\n",
       " 'MYSELF',\n",
       " 'NO',\n",
       " 'NOR',\n",
       " 'NOT',\n",
       " 'NOW',\n",
       " 'OF',\n",
       " 'OFF',\n",
       " 'ON',\n",
       " 'ONCE',\n",
       " 'ONLY',\n",
       " 'OR',\n",
       " 'OTHER',\n",
       " 'OUR',\n",
       " 'OURS',\n",
       " 'OURSELVES',\n",
       " 'OUT',\n",
       " 'OVER',\n",
       " 'OWN',\n",
       " 'SAME',\n",
       " 'SHE',\n",
       " 'SHOULD',\n",
       " 'SO',\n",
       " 'SOME',\n",
       " 'SUCH',\n",
       " 'THAN',\n",
       " 'THAT',\n",
       " 'THE',\n",
       " 'THEIR',\n",
       " 'THEIRS',\n",
       " 'THEM',\n",
       " 'THEMSELVES',\n",
       " 'THEN',\n",
       " 'THERE',\n",
       " 'THESE',\n",
       " 'THEY',\n",
       " 'THIS',\n",
       " 'THOSE',\n",
       " 'THROUGH',\n",
       " 'TO',\n",
       " 'TOO',\n",
       " 'UNDER',\n",
       " 'UNTIL',\n",
       " 'UP',\n",
       " 'VERY',\n",
       " 'WAS',\n",
       " 'WE',\n",
       " 'WERE',\n",
       " 'WHAT',\n",
       " 'WHEN',\n",
       " 'WHERE',\n",
       " 'WHICH',\n",
       " 'WHILE',\n",
       " 'WHO',\n",
       " 'WHOM',\n",
       " 'WHY',\n",
       " 'WITH',\n",
       " 'YOU',\n",
       " 'YOUR',\n",
       " 'YOURS',\n",
       " 'YOURSELF',\n",
       " 'YOURSELVES',\n",
       " 'a',\n",
       " \"a's\",\n",
       " 'able',\n",
       " 'about',\n",
       " 'above',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'across',\n",
       " 'actually',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " \"ain't\",\n",
       " 'all',\n",
       " 'allow',\n",
       " 'allows',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'anywhere',\n",
       " 'apart',\n",
       " 'appear',\n",
       " 'appreciate',\n",
       " 'appropriate',\n",
       " 'are',\n",
       " \"aren't\",\n",
       " 'around',\n",
       " 'as',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asking',\n",
       " 'associated',\n",
       " 'at',\n",
       " 'available',\n",
       " 'away',\n",
       " 'awfully',\n",
       " 'b',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'believe',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'best',\n",
       " 'better',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'both',\n",
       " 'brief',\n",
       " 'but',\n",
       " 'by',\n",
       " 'c',\n",
       " \"c'mon\",\n",
       " \"c's\",\n",
       " 'came',\n",
       " 'can',\n",
       " \"can't\",\n",
       " 'cannot',\n",
       " 'cant',\n",
       " 'cause',\n",
       " 'causes',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'changes',\n",
       " 'clearly',\n",
       " 'co',\n",
       " 'com',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'concerning',\n",
       " 'consequently',\n",
       " 'consider',\n",
       " 'considering',\n",
       " 'contain',\n",
       " 'containing',\n",
       " 'contains',\n",
       " 'corresponding',\n",
       " 'could',\n",
       " \"couldn't\",\n",
       " 'course',\n",
       " 'currently',\n",
       " 'd',\n",
       " 'definitely',\n",
       " 'described',\n",
       " 'despite',\n",
       " 'did',\n",
       " \"didn't\",\n",
       " 'different',\n",
       " 'do',\n",
       " 'does',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " \"don't\",\n",
       " 'done',\n",
       " 'down',\n",
       " 'downwards',\n",
       " 'during',\n",
       " 'e',\n",
       " 'each',\n",
       " 'edu',\n",
       " 'eg',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'enough',\n",
       " 'entirely',\n",
       " 'especially',\n",
       " 'et',\n",
       " 'etc',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everybody',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'ex',\n",
       " 'exactly',\n",
       " 'example',\n",
       " 'except',\n",
       " 'f',\n",
       " 'far',\n",
       " 'few',\n",
       " 'fifth',\n",
       " 'first',\n",
       " 'five',\n",
       " 'followed',\n",
       " 'following',\n",
       " 'follows',\n",
       " 'for',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forth',\n",
       " 'four',\n",
       " 'from',\n",
       " 'further',\n",
       " 'furthermore',\n",
       " 'g',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'given',\n",
       " 'gives',\n",
       " 'go',\n",
       " 'goes',\n",
       " 'going',\n",
       " 'gone',\n",
       " 'got',\n",
       " 'gotten',\n",
       " 'greetings',\n",
       " 'h',\n",
       " 'had',\n",
       " \"hadn't\",\n",
       " 'happens',\n",
       " 'hardly',\n",
       " 'has',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he's\",\n",
       " 'hello',\n",
       " 'help',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " \"here's\",\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'hi',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'hither',\n",
       " 'hopefully',\n",
       " 'how',\n",
       " 'howbeit',\n",
       " 'however',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'ie',\n",
       " 'if',\n",
       " 'ignored',\n",
       " 'immediate',\n",
       " 'in',\n",
       " 'inasmuch',\n",
       " 'inc',\n",
       " 'indeed',\n",
       " 'indicate',\n",
       " 'indicated',\n",
       " 'indicates',\n",
       " 'inner',\n",
       " 'insofar',\n",
       " 'instead',\n",
       " 'into',\n",
       " 'inward',\n",
       " 'is',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'j',\n",
       " 'just',\n",
       " 'k',\n",
       " 'keep',\n",
       " 'keeps',\n",
       " 'kept',\n",
       " 'know',\n",
       " 'knows',\n",
       " 'known',\n",
       " 'l',\n",
       " 'last',\n",
       " 'lately',\n",
       " 'later',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'lest',\n",
       " 'let',\n",
       " \"let's\",\n",
       " 'like',\n",
       " 'liked',\n",
       " 'likely',\n",
       " 'little',\n",
       " 'look',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'ltd',\n",
       " 'm',\n",
       " 'mainly',\n",
       " 'many',\n",
       " 'may',\n",
       " 'maybe',\n",
       " 'me',\n",
       " 'mean',\n",
       " 'meanwhile',\n",
       " 'merely',\n",
       " 'might',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'n',\n",
       " 'name',\n",
       " 'namely',\n",
       " 'nd',\n",
       " 'near',\n",
       " 'nearly',\n",
       " 'necessary',\n",
       " 'need',\n",
       " 'needs',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'new',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'non',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'normally',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'novel',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'o',\n",
       " 'obviously',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'oh',\n",
       " 'ok',\n",
       " 'okay',\n",
       " 'old',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'ones',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'ought',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'outside',\n",
       " 'over',\n",
       " 'overall',\n",
       " 'own',\n",
       " 'p',\n",
       " 'particular',\n",
       " 'particularly',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'placed',\n",
       " 'please',\n",
       " 'plus',\n",
       " 'possible',\n",
       " 'presumably',\n",
       " 'probably',\n",
       " 'provides',\n",
       " 'q',\n",
       " 'que',\n",
       " 'quite',\n",
       " 'qv',\n",
       " 'r',\n",
       " 'rather',\n",
       " 'rd',\n",
       " 're',\n",
       " 'really',\n",
       " 'reasonably',\n",
       " 'regarding',\n",
       " 'regardless',\n",
       " 'regards',\n",
       " 'relatively',\n",
       " 'respectively',\n",
       " 'right',\n",
       " 's',\n",
       " 'said',\n",
       " 'same',\n",
       " 'saw',\n",
       " 'say',\n",
       " 'saying',\n",
       " 'says',\n",
       " 'second',\n",
       " 'secondly',\n",
       " 'see',\n",
       " 'seeing',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'seen',\n",
       " 'self',\n",
       " 'selves',\n",
       " 'sensible',\n",
       " 'sent',\n",
       " 'serious',\n",
       " 'seriously',\n",
       " 'seven',\n",
       " 'several',\n",
       " 'shall',\n",
       " 'she',\n",
       " 'should',\n",
       " \"shouldn't\",\n",
       " 'since',\n",
       " 'six',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somebody',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhat',\n",
       " 'somewhere',\n",
       " 'soon',\n",
       " 'sorry',\n",
       " 'specified',\n",
       " 'specify',\n",
       " 'specifying',\n",
       " 'still',\n",
       " 'sub',\n",
       " 'such',\n",
       " 'sup',\n",
       " 'sure',\n",
       " 't',\n",
       " \"t's\",\n",
       " 'take',\n",
       " 'taken',\n",
       " 'tell',\n",
       " 'tends',\n",
       " 'th',\n",
       " 'than',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'thanx',\n",
       " 'that',\n",
       " \"that's\",\n",
       " 'thats',\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " \"there's\",\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " 'theres',\n",
       " 'thereupon',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'think',\n",
       " 'third',\n",
       " 'this',\n",
       " 'thorough',\n",
       " 'thoroughly',\n",
       " 'those',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'together',\n",
       " 'too',\n",
       " 'took',\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f024b926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e283a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"Ranking customer behaviours for business strategy Algorithmic trading for multiple commodities markets, like Forex, Metals, Energy, etc. Trading Bot for FOREX Python model for the analysis of sector-specific stock ETFs for investment purposes Playstore & Appstore to Google Analytics (GA) or Firebase to Google Data Studio Mobile App KPI Dashboard Google Local Service Ads LSA API To Google BigQuery to Google Data Studio AI Conversational Bot using RASA Recommendation System Architecture Rise of telemedicine and its Impact on Livelihood by 2040 Rise of e-health and its impact on humans by the year 2030 Rise of e-health and its impact on humans by the year 2030 Rise of Chatbots and its impact on customer support by the year 2040 AI/ML and Predictive Modeling Solution for Contact Centre Problems How to Setup Custom Domain for Google App Engine Application? Code Review Checklist Ranking customer behaviours for business strategy Algorithmic trading for multiple commodities markets, like Forex, Metals, Energy, etc. Trading Bot for FOREX Python model for the analysis of sector-specific stock ETFs for investment purposes Playstore & Appstore to Google Analytics (GA) or Firebase to Google Data Studio Mobile App KPI Dashboard Google Local Service Ads LSA API To Google BigQuery to Google Data Studio AI Conversational Bot using RASA Recommendation System Architecture Rise of telemedicine and its Impact on Livelihood by 2040 Rise of e-health and its impact on humans by the year 2030 Rise of e-health and its impact on humans by the year 2030 Rise of Chatbots and its impact on customer support by the year 2040 AI/ML and Predictive Modeling Solution for Contact Centre Problems How to Setup Custom Domain for Google App Engine Application? Code Review Checklist In future or in upcoming years humans and machines are going to work together in every field of work. In upcoming days machines will be the need for every human being. Machines [AI technology] will do the work which humans are incapable of doing. Machines will partner and co-operate with humans. According to the professor at the university of Washington, he explained that, as a result of AI, there will be more demand for existing jobs and new jobs will be created that are unimaginable today. Human workers and machines will work together flawlessly, complementing each other. Machines will learn to carry out easier tasks such as following processes or crunching data. They will also help the humans while difficult. Machines or AI will create a great job opportunities for humans in future. John Kelly ll, executive vice president of IBM once said that “Man and Machines working together always beat or make a better decision than a man or a machine independently.”  In future, the three sectors of our country like agriculture sector, industrial sector and service sector are going to utilize the machines. So, that their work becomes not difficult. As of now, we can only see that for agriculture purposes various kinds of machines are used which we called as a modern farming method. Some major technologies [machines] that are harvest automation, autonomous tractors, seeding, and weeing and drones. As a result, farms can do agriculture peacefully. In the industrial sector also humans and machines are working together to increase production. Various types of machines are used in industries such as packing machines, loading machine etc. humans provide instructions to the machines and maintain the management in the company. Soon robots [machines] will assist doctors with surgeries. For instance, a doctor at remote location could direct a surgical robot to perform an open heart surgery. But the approaches option and decision will be left to experience and wisdom of the doctor not the robot. What do you think of machines if they will make humans less or more in the field? Machines will push human professionals up the skillset ladder into uniquely human skills such as creativity, social abilities, empathy, and sense-making, which machines cannot automate. As a result, machines will make the workplace more, not less for humans. However, humans have to learn new skills throughout their lives. It is said that in the future 80% of process-oriented tasks will be done by machines. Quantitative reasoning tasks will be done approximately 50% by humans and 50% by machines, while humans will continue to do more than 80% of cross-functional reasoning tasks. According to Harvard research machines, algorithms can read diagnostic scans with 92% accuracy. Humans can do it with 96% accuracy. Together, it will be 99% accurate. Human-machine collaboration enables companies to interact with employees and customers in the novel, more effective ways. Smart machines are helping humans to expand their abilities in three ways. They can amplify our cognitive strengths; interact with customers and employees to free us from higher-level tasks, and embody human skills to extend our physical capabilities. In the research, it was found that 1,500 companies achieve the most significant performance improvement when humans and machines work together. New machine systems have beyond-human cognitive abilities, which many of us fear could potentially dehumanize the future of work. Machines will indeed automate most repetitive and physical tasks, and part of quantitative tasks such as programming and even data science. According to D.E Shaw Group and professor at the University of Washington, explained that, as a result of machines, there will be more demand for existing jobs, and new jobs will be created that are unimaginable today. This is similar to how we couldn’t imagine a web app developer decades ago, and now millions make a living doing that today. Machines are good at doing tasks with speed, precision, and accuracy. But machines are not very good at responding to unknown situations or making judgments. That part will be left to humans. Hence, the need for both humans and machines will be there in the future. Humans and machines have divergent skill sets that, when combined can transform the way we work. Machines have already infiltrated every aspect of our lives, and we must learn to live with them. In the future, human workers will interact more closely with humans.                           Newspaper is your news, entertainment, music fashion website. We provide you with the latest breaking news and videos straight from the entertainment industry. Contact us: contact@yoursite.com © Newspaper WordPress Theme by TagDiv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cf8322c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'process_articles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m temp \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_articles\u001b[49m(article, stopwords_articles)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'process_articles' is not defined"
     ]
    }
   ],
   "source": [
    "temp = process_articles(article, stopwords_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecd82c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be0995fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '../scrapped_data_folder'\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "285053e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        url_id = os.path.splitext(filename)[0]\n",
    "        data.append((url_id, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7c6e309",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = pd.DataFrame(data, columns=['URL_ID', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c1553b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>Estimating the impact of COVID-19 on the world...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>Travel and Tourism Outlook Ranking customer be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102</td>\n",
       "      <td>Gaming Disorder and Effects of Gaming on Healt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103</td>\n",
       "      <td>What is the repercussion of the environment du...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>104</td>\n",
       "      <td>Due to the COVID-19 the repercussion of the en...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  URL_ID                                               text\n",
       "0    100  Estimating the impact of COVID-19 on the world...\n",
       "1    101  Travel and Tourism Outlook Ranking customer be...\n",
       "2    102  Gaming Disorder and Effects of Gaming on Healt...\n",
       "3    103  What is the repercussion of the environment du...\n",
       "4    104  Due to the COVID-19 the repercussion of the en..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c84bec5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31738eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_url = pd.read_excel('../input.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfaab62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text['URL_ID'] = df_text['URL_ID'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c45eb4c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>Estimating the impact of COVID-19 on the world...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>Travel and Tourism Outlook Ranking customer be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102</td>\n",
       "      <td>Gaming Disorder and Effects of Gaming on Healt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103</td>\n",
       "      <td>What is the repercussion of the environment du...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>104</td>\n",
       "      <td>Due to the COVID-19 the repercussion of the en...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                               text\n",
       "0     100  Estimating the impact of COVID-19 on the world...\n",
       "1     101  Travel and Tourism Outlook Ranking customer be...\n",
       "2     102  Gaming Disorder and Effects of Gaming on Healt...\n",
       "3     103  What is the repercussion of the environment du...\n",
       "4     104  Due to the COVID-19 the repercussion of the en..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3efb6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df_url, df_text, on = \"URL_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "600919eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "      <td>AI in healthcare to Improve Patient Outcomes R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "      <td>What if the Creation is Taking Over the Creato...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "      <td>What Jobs Will Robots Take From Humans in The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "      <td>Will Machine Replace The Human in the Future o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "      <td>Will AI Replace Us or Work With Us? Ranking cu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL  \\\n",
       "0      37  https://insights.blackcoffer.com/ai-in-healthc...   \n",
       "1      38  https://insights.blackcoffer.com/what-if-the-c...   \n",
       "2      39  https://insights.blackcoffer.com/what-jobs-wil...   \n",
       "3      40  https://insights.blackcoffer.com/will-machine-...   \n",
       "4      41  https://insights.blackcoffer.com/will-ai-repla...   \n",
       "\n",
       "                                                text  \n",
       "0  AI in healthcare to Improve Patient Outcomes R...  \n",
       "1  What if the Creation is Taking Over the Creato...  \n",
       "2  What Jobs Will Robots Take From Humans in The ...  \n",
       "3  Will Machine Replace The Human in the Future o...  \n",
       "4  Will AI Replace Us or Work With Us? Ranking cu...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5e30401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_articles(article, stopwords_articles):\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    article = re.sub(r'\\$\\w*', '', article)\n",
    "    article = re.sub(r'^RT[\\s]+', '', article)\n",
    "    article = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', article)\n",
    "    article = re.sub(r'#', '', article)\n",
    "    article = article.lower()\n",
    "    tokenizer = nltk.TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    article_tokens = tokenizer.tokenize(article)\n",
    "\n",
    "    article_clean = []\n",
    "    for word in article_tokens:\n",
    "        if (word not in stopwords_articles and word not in string.punctuation):\n",
    "            stem_word = stemmer.stem(word)\n",
    "            article_clean.append(stem_word)\n",
    "    article_clean=' '.join(article_clean)\n",
    "    return article_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66d9ca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_cleaned']=df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f897b1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a\\AppData\\Local\\Temp\\ipykernel_11972\\2865773252.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text_cleaned'][i] = process_articles(df['text'][i], stopwords_articles)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(df)):\n",
    "    df['text_cleaned'][i] = process_articles(df['text_cleaned'][i], stopwords_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "257d3a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>text</th>\n",
       "      <th>text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "      <td>AI in healthcare to Improve Patient Outcomes R...</td>\n",
       "      <td>ai healthcar improv patient outcom rank custom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "      <td>What if the Creation is Taking Over the Creato...</td>\n",
       "      <td>creation take creator rank custom behaviour bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "      <td>What Jobs Will Robots Take From Humans in The ...</td>\n",
       "      <td>job robot human futur rank custom behaviour bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "      <td>Will Machine Replace The Human in the Future o...</td>\n",
       "      <td>machin replac human futur work rank custom beh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "      <td>Will AI Replace Us or Work With Us? Ranking cu...</td>\n",
       "      <td>ai replac work rank custom behaviour busi stra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL  \\\n",
       "0      37  https://insights.blackcoffer.com/ai-in-healthc...   \n",
       "1      38  https://insights.blackcoffer.com/what-if-the-c...   \n",
       "2      39  https://insights.blackcoffer.com/what-jobs-wil...   \n",
       "3      40  https://insights.blackcoffer.com/will-machine-...   \n",
       "4      41  https://insights.blackcoffer.com/will-ai-repla...   \n",
       "\n",
       "                                                text  \\\n",
       "0  AI in healthcare to Improve Patient Outcomes R...   \n",
       "1  What if the Creation is Taking Over the Creato...   \n",
       "2  What Jobs Will Robots Take From Humans in The ...   \n",
       "3  Will Machine Replace The Human in the Future o...   \n",
       "4  Will AI Replace Us or Work With Us? Ranking cu...   \n",
       "\n",
       "                                        text_cleaned  \n",
       "0  ai healthcar improv patient outcom rank custom...  \n",
       "1  creation take creator rank custom behaviour bu...  \n",
       "2  job robot human futur rank custom behaviour bu...  \n",
       "3  machin replac human futur work rank custom beh...  \n",
       "4  ai replac work rank custom behaviour busi stra...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d096308",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data_folder/cleaned_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf50cdec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae9cb406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_positive_score(text):\n",
    "    \"\"\"\n",
    "    Calculates the positive score of a given text using the Positive Dictionary.\n",
    "    \"\"\"\n",
    "    with open('../MasterDictionary/positive-words.txt') as f:\n",
    "        positive_words = set(f.read().split())\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    positive_score = sum(1 for token in tokens if token in positive_words)\n",
    "    return positive_score\n",
    "\n",
    "\n",
    "def calculate_negative_score(text):\n",
    "    \"\"\"\n",
    "    Calculates the negative score of a given text using the Negative Dictionary.\n",
    "    \"\"\"\n",
    "    with open('../MasterDictionary/negative-words.txt') as f:\n",
    "        negative_words = set(f.read().split())\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    negative_score = sum(1 for token in tokens if token in negative_words)\n",
    "    return -1 * negative_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e817166",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"ai healthcar improv patient outcom rank custom behaviour busi strategi algorithm trade multipl commod market forex metal energi trade bot forex python model analysi sector-specif stock etf invest purpos playstor appstor googl analyt ga firebas googl data studio mobil app kpi dashboard googl local servic ad lsa api googl bigqueri googl data studio ai convers bot rasa recommend system architectur rise telemedicin impact livelihood 2040 rise e-health impact human year 2030 rise e-health impact human year 2030 rise chatbot impact custom support year 2040 ai ml predict model solut contact centr problem setup custom domain googl app engin applic code review checklist rank custom behaviour busi strategi algorithm trade multipl commod market forex metal energi trade bot forex python model analysi sector-specif stock etf invest purpos playstor appstor googl analyt ga firebas googl data studio mobil app kpi dashboard googl local servic ad lsa api googl bigqueri googl data studio ai convers bot rasa recommend system architectur rise telemedicin impact livelihood 2040 rise e-health impact human year 2030 rise e-health impact human year 2030 rise chatbot impact custom support year 2040 ai ml predict model solut contact centr problem setup custom domain googl app engin applic code review checklist introduct â€œ kill 10 million peopl decad highli infecti viru war missil microb â€ bill gate â€™ remark ted confer 2014 world avoid ebola outbreak unpreced invis viru hit met overwhelm unprepar healthcar system oblivi popul public health emerg demonstr lack scientif consider underlin alarm robust innov health medic facil past year artifici intellig proven tangibl potenti healthcar sector clinic practic translat medic biomed research case detect china decemb 31st 2019 ai program develop bluedot alert world pandem quick realis ai â€™ abil analys larg chunk data detect pattern identifi track carrier viru trace app ai tab peopl infect prevent risk cross-infect ai algorithm track pattern extract featur classifi categoris ai ibm watson sophist ai work cloud comput natur languag process promin contribut healthcar sector global level convers ai 2013 watson help recommend treatment patient suffer cancer ensur treatment optimum cost research googl show ai system train thousand imag achiev physician-level sensit identifi molecular pattern diseas statu subtyp gene express protein abund level machin learn method detect fatal diseas cancer earli stage machin learn ml techniqu focu analyz structur data cluster patient â€™ trait infer probabl diseas outcom patient trait includ mass data relat age gender diseas histori disease-specif data diagnost imag gene express ml extract featur data input construct data analyt algorithm ml algorithm supervis unsupervis unsupervis learn help extract featur cluster similar featur lead earli detect diseas cluster princip compon analysi enabl group cluster similar trait maxim minim similar patient cluster patient trait record multipl dimens gene princip compon analysi pca creat apparatu reduc dimens human supervis learn consid outcom subject trait correl input output predict probabl clinic event expect diseas level expect surviv time risk â€™ syndrom biomark panel detect ovarian cancer outperform convent statist method due machin learn addit ehr bayesian network part supervis machin learn algorithm predict clinic outcom mortal unstructur data clinic note text convert machine-read structur data natur languag process nlp nlp work compon text process classif text process help identifi seri disease-relev keyword clinic note classif categor normal abnorm case chest screen ml nlp help find abnorm lung provid treatment covid patient healthcar organ nlp-base chatbot increas interact patient keep mental health well check deep learn modern extens classic neural network techniqu help explor complex non-linear pattern data algorithm convolut neural network recurr neural network deep belief network deep neural network enabl accur clinic predict genom interpret deep neural network surpass convent method logist regress support vector machin sepsi watch ai system train deep learn algorithm hold capabl analyz 32 million data point creat patient â€™ risk score identifi earli stage sepsi method learning-bas optim sampl pattern loup base integr full resolut mri scan convolut neural network algorithm help creat accur reconstruct robot surgeri wide consid delic surgeri gynaecolog prostat surgeri strike balanc human decis ai precis robot surgeri reduc surgeon effici manual oper consol autonom robot surgeri rise invent robot silicon finger mimic sens touch surgeon identifi organ cut tissu robot cathet navig touch blood tissu valv research children â€™ nation hospit washington develop ai call smart tissu autonom robot star perform colon anastomosi ml-power sutur tool automat detect patient â€™ breath pattern appli sutur correct point cloud comput healthcar help retriev share medic record safe reduct mainten cost technolog doctor healthcar worker access detail patient data help speed analysi ultim lead care form accur inform medic therapi biomed research ai analyz literatur readabl concis biomed research ml algorithm nlp ai acceler screen index biomed research rank literatur interest research formul test scientif hypothes precis quickli take level ai system comput model assist cma help research construct simul model concept mind innov majorli contribut topic tumour suppressor mechan protein-protein interact inform extract ai precis medicin precis medicin focus healthcar intervent individu group patient base profil ai devic pave practic effici ml complex algorithm larg dataset predict creat optim treatment strategi deep learn neural network process data healthcar app close watch patient â€™ emot state food intak health monitor â€œ omic â€ refer collect technolog explor role relationship branch end suffix â€œ omic â€ genom proteom omics-bas test base machin learn algorithm find correl predict treatment respons ultim creat person treatment individu patient help psycholog neuro patient psychologist studi creativ ai promis class experi develop data structur program explor theori horizon studi show ai conduct therapi session e-therapi session assess autonom assist human practition session detect comput analysi psycholog signal project ml comput vision nlp analyz languag physic gestur social signal identifi cue human distress ground-break technolog assess soldier return combat recogn requir mental health support futur combin data captur face-to-fac interview inform sleep eat onlin behaviour complet patient view stroke identif stroke frequent occur diseas affect 500 million peopl worldwid thrombu vessel cerebr infarct major 85 stroke occurr recent year ai techniqu numer stroke-rel studi earli detect time treatment effici outcom predict solv problem ai dispos larg amount data rich inform complic real-lif clinic question address arena ml algorithm genet fuzzi finit state machin pca implement build model build solut includ human activ recognit stage stroke onset detect stage alert stroke messag activ movement significantli normal pattern record ml method appli neuroimag data assist diseas evalu predict stroke treatment diagnosi patient monitor today market ai-bas patient monitor impress monetarili entic evolv artifici sensor smart technolog explor brain-comput interfac nanorobot compani smart-watch engag peopl perform remot monitor â€œ patient â€ obviou place start wearabl embed sensor glucos monitor puls monitor oximet ecg monitor patient monitor crucial ai find numer applic chronic condit intens care unit oper room emerg room cardiac ward timeless clinic decision-mak measur second advanc start gain traction smart prosthet implant play impecc role patient manag post-surgeri rehabilit demograph laboratori result vital sign predict cardiac arrest transfer intens care unit death addit interpret machine-learn model assist anesthesiologist predict hypoxaemia event surgeri suggest deep-learn algorithm raw patient-monitor data avoid inform overload alert overload enabl accur clinic predict time decision-mak conclus vast rang task ai evid hold deep potenti improv patient outcom skyrocket level sophist algorithm ai bring revolut healthcar sector face challeng technolog deliv promis ethic measur train physician standard regul role ai transform clinic practic biggest challeng integr ai daili practic overcom period technolog matur make system enhanc effect newspap news entertain music fashion websit provid latest break news video straight entertain industri contact contact@yoursite.com Â© newspap wordpress theme tagdiv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c675916d",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\a/nltk_data'\n    - 'D:\\\\Files\\\\Blackcloffer\\\\env\\\\nltk_data'\n    - 'D:\\\\Files\\\\Blackcloffer\\\\env\\\\share\\\\nltk_data'\n    - 'D:\\\\Files\\\\Blackcloffer\\\\env\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\a\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcalculate_negative_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[33], line 18\u001b[0m, in \u001b[0;36mcalculate_negative_score\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../MasterDictionary/negative-words.txt\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     17\u001b[0m     negative_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(f\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplit())\n\u001b[1;32m---> 18\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m negative_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m negative_words)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m*\u001b[39m negative_score\n",
      "File \u001b[1;32mD:\\Files\\Blackcloffer\\env\\lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32mD:\\Files\\Blackcloffer\\env\\lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mD:\\Files\\Blackcloffer\\env\\lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mD:\\Files\\Blackcloffer\\env\\lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[1;32mD:\\Files\\Blackcloffer\\env\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\a/nltk_data'\n    - 'D:\\\\Files\\\\Blackcloffer\\\\env\\\\nltk_data'\n    - 'D:\\\\Files\\\\Blackcloffer\\\\env\\\\share\\\\nltk_data'\n    - 'D:\\\\Files\\\\Blackcloffer\\\\env\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\a\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "calculate_negative_score(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c65e613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_avg_words_per_sentence(text):\n",
    "    # Tokenize sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Calculate average number of words per sentence\n",
    "    num_words = len(word_tokenize(text))\n",
    "    avg_words_per_sentence = num_words / len(sentences)\n",
    "    \n",
    "    return avg_words_per_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "160a68fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\a/nltk_data'\n    - 'D:\\\\Files\\\\Blackcloffer\\\\env\\\\nltk_data'\n    - 'D:\\\\Files\\\\Blackcloffer\\\\env\\\\share\\\\nltk_data'\n    - 'D:\\\\Files\\\\Blackcloffer\\\\env\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\a\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcalc_avg_words_per_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[38], line 3\u001b[0m, in \u001b[0;36mcalc_avg_words_per_sentence\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalc_avg_words_per_sentence\u001b[39m(text):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Tokenize sentences\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Calculate average number of words per sentence\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     num_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(word_tokenize(text))\n",
      "File \u001b[1;32mD:\\Files\\Blackcloffer\\env\\lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mD:\\Files\\Blackcloffer\\env\\lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mD:\\Files\\Blackcloffer\\env\\lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[1;32mD:\\Files\\Blackcloffer\\env\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\a/nltk_data'\n    - 'D:\\\\Files\\\\Blackcloffer\\\\env\\\\nltk_data'\n    - 'D:\\\\Files\\\\Blackcloffer\\\\env\\\\share\\\\nltk_data'\n    - 'D:\\\\Files\\\\Blackcloffer\\\\env\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\a\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "calc_avg_words_per_sentence(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92759bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
